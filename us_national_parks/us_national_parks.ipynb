{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the National Parks of the United States\n",
    "> \"Wilderness is not a luxury but a necessity of the human spirit, and as vital to our lives as water and good bread. A civilization which destroys what little remains of the wild, the spare, the original, is cutting itself off from its origins and betraying the principle of civilization itself.\" <br>***Edward Abbey***, [Desert Solitare](https://www.goodreads.com/book/show/214614.Desert_Solitaire?ac=1&from_search=true&qid=3inpz3msB9&rank=1)<br><br>\n",
    "\"Like winds and sunsets, wild things were taken for granted until progress began to do away with them. Now, we face the question whether a still higher 'standard of living' is worth its cost in things natural, wild and free.\" <br>***Aldo Leopold***, [A Sand County Almanac](https://www.aldoleopold.org/about/aldo-leopold/sand-county-almanac/)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [wiki](https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States): \"The United States has 63 national parks, which are congressionally designated protected areas operated by the National Park Service, an agency of the Department of the Interior. National parks are designated for their natural beauty, unique geological features, diverse ecosystems, and recreational opportunities. The [Organic Act of 1916](https://www.nps.gov/grba/learn/management/organic-act-of-1916.htm) created the National Park Service 'to conserve the scenery and the natural and historic objects and wildlife therein, and to provide for the enjoyment of the same in such manner and by such means as will leave them unimpaired for the enjoyment of future generations.' \"<br><br>\n",
    "**GOAL:** *(1)* Characterize reactional use of public lands in the United States; and *(2)* learn how to scrape and visualize data from a webpage.<br>\n",
    "**DATA:** Publicly avaialable data on recreational use of the US national parks will be scrapped from the [National Parks Service (NPS)](https://irma.nps.gov/STATS/) website.<br>\n",
    "**ANALYSIS:** Exploratory data analysis to gain insights into the dataset.<br>\n",
    "**ETHICAL CONSIDERATIONS:** There are no apparent issues with privacy, transparency,\n",
    "or accountability in terms of avaiable data. Whether access to the national parks is equitable across communities in the US should be considered further. The NPS has begun administering a [survey](https://www.nps.gov/subjects/socialscience/socioeconomic-monitoring-visitor-surveys.htm) to understand who accesses the parks, and whether access differs as a function of demographic and economic factors. I'd like to incorporate data from that survey into this notebook when it becomes publicly available.<br>\n",
    "**ADDITIONAL CONSIDERATIONS:** None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from keplergl import KeplerGl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create parks data frame \n",
    "First step is to compile a data frame of the national parks in the United States. For practice, I will scrape this information from this [wiki page](https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to scrape information from\n",
    "wiki = 'https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract HTML text\n",
    "page = rq.get(wiki).text\n",
    "\n",
    "# convert to BeautifulSoup object\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pull <table> tag that match our class name\n",
    "table = soup.find('table', class_='wikitable sortable plainrowheaders')\n",
    "\n",
    "# find <tr> tags in our specified table, ignoring the labels row\n",
    "parks_table = table.find_all('tr')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create empty list to store park information\n",
    "parks_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through each park in the website's table\n",
    "for park in parks_table:\n",
    "    \n",
    "    # extract name from <a> tag\n",
    "    name = park.find('a').get('title')\n",
    "\n",
    "    # extract state/teritory from specific <a> tag\n",
    "    state_terr = park.find_all('a')[2].get('title')\n",
    "    \n",
    "    # extract date established from specific <span> tag\n",
    "    established = park.find_all('span')[13].get('data-sort-value')\n",
    "    established = pd.to_datetime(established[8:-5]).date()\n",
    "                                \n",
    "    # extract park latitude from <span class='latitude'> tag\n",
    "    latitude = park.find(attrs={'class':'span', 'class':'latitude'})\n",
    "    latitude = latitude.text[:-2].replace('°', '.')\n",
    "    \n",
    "    # extract park longitude from <span class='longitude'> tag\n",
    "    longitude = park.find(attrs={'class':'span', 'class':'longitude'})\n",
    "    longitude = longitude.text[:-2].replace('°', '.')\n",
    "    \n",
    "    # append information to full parks dataframe\n",
    "    parks_list.append([name, state_terr, latitude, longitude, established])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of NPS park abbreviations\n",
    "parks_abrv = ['ACAD', 'NPAS', 'ARCH', 'BADL', 'BIBE', 'BISC', 'BLCA', 'BRCA', 'CANY', 'CARE', \n",
    "              'CAVE', 'CHIS', 'CONG', 'CRLA', 'CUVA', 'DEVA', 'DENA', 'DRTO', 'EVER', 'GAAR', \n",
    "              'JEFF', 'GLBA', 'GLAC', 'GRCA', 'GRTE', 'GRBA', 'GRSA', 'GRSM', 'GUMO', 'HALE', \n",
    "              'HAVO', 'HOSP', 'INDU', 'ISRO', 'JOTR', 'KATM', 'KEFJ', 'KICA', 'KOVA', 'LACL', \n",
    "              'LAVO', 'MACA', 'MEVE', 'MORA', 'NERI', 'NOCA', 'OLYM', 'PEFO', 'PINN', 'REDW', \n",
    "              'ROMO', 'SAGU', 'SEQU', 'SHEN', 'THRO', 'VIIS', 'VOYA', 'WHSA', 'WICA', 'WRST', \n",
    "              'YELL', 'YOSE', 'ZION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append NPS abbrevations to parks list\n",
    "for x, y in zip(parks_list, parks_abrv):\n",
    "    x.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create parks dataframe\n",
    "parks = pd.DataFrame(parks_list,\n",
    "                     columns=['name', 'state_terr', 'latitude', 'longitude', 'est_date', 'nps_abrv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parks.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a data frame with the name, state/territory, coordinates, establishment date, and abbreviation for each national parks in the United States. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of parks\n",
    "The next step will be to use `kepler.gl` to visualize the locations of each national park. I'd like to do something similar to [this approach](https://www.kaggle.com/code/parulpandey/visualizing-india-s-seismic-activity/notebook) posted on Kaggle. Let's come back to this another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_1 = KeplerGl(height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#map_1.add_data(data=parks, name='name')\n",
    "#map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape NPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to scrape information from\n",
    "nps = 'https://irma.nps.gov/STATS/SSRSReports/Park%20Specific%20Reports/Recreation%20Visitors%20By%20Month%20(1979%20-%20Last%20Calendar%20Year)?Park={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to store park data\n",
    "#parks_data = []\n",
    "\n",
    "# temporary parks list for testing\n",
    "test_park = ['ACAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for park in test_park:\n",
    "    \n",
    "    # url to monthly data for each park \n",
    "    url = nps.format(park)\n",
    "    \n",
    "    # extract HTML text\n",
    "    page = rq.get(url).text\n",
    "    print(page)\n",
    "    \n",
    "    # convert to BeautifulSoup object\n",
    "    #soup = BeautifulSoup(page)\n",
    "    \n",
    "    # pull <table> tag that match our class name\n",
    "    #table = soup.find('table', class_ = 'Ae30f20f368af4927806ac09a734045d0170')\n",
    "    \n",
    "    # find <tr> tags in our specified table, ignoring the labels row\n",
    "    #temp_table = table.find_all('tr')\n",
    "    \n",
    "    # extract park name from <a> tag\n",
    "    #xyz = park.find('a').get('xyz')\n",
    "    \n",
    "    # append information to full parks list\n",
    "    #park_data.append(xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be an issue with extracting the HTML text from the webpage. Maybe that's due to javascript? Unclear. There is another popular web-scraping tool called [Selenium](https://oxylabs.io/blog/selenium-web-scraping) that is suppose to work with javascript. I'll try that out next.<br><br>***To be continued***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Zwift\n",
    "\n",
    "> \"Every time I see an adult on a bicycle, I no longer despair for the future of the human race.\" \n",
    "<br>**H. G. Wells**<br><br>\n",
    "“Learn to ride a bicycle. You will not regret it if you live.” \n",
    "<br>**Mark Twain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've mostly biked indoors over the past few years, due to COVID and a shoulder injury. I joined [Zwift](https://us.zwift.com/) last year on the recommendation of a friend. Zwift gamifies bike training, offering an array of virtual routes that vary in difficulty. I'm currently using the app to train for a bikepack trip [~1400 miles along the Pacific Coast](https://www.adventurecycling.org/routes-and-maps/adventure-cycling-route-network/pacific-coast/) next summer. \n",
    "\n",
    "One of Zwift's best features is their ability to create insightful visualizations from user data, acquired from a bluetooth-enabled bike trainer. I recently learned that users can download their ride data, providing an opportunity to further characterize my training experience.<br><br>\n",
    "\n",
    "**GOAL:** *(1)* Characterize bike training patterns over time, starting from March 1, 2022; and *(2)* describe anticipated level of functioning on May 1, 2023.<br>\n",
    "**DATA:** Ride data downloaded from personal Zwift (v5.62) account.<br>\n",
    "**ANALYSIS:** Exploratory data analysis; Bayesian modeling.<br>\n",
    "**ETHICAL CONSIDERATIONS:** There are no apparent issues with transparency, accountability, or equity in terms of avaiable data. To avoid any unforseen privacy issues, I will not be posting the raw ride data on Github. I will do my best to characterize the data in this Notebook to justify any insights drawn from the analyses.<br>\n",
    "**ADDITIONAL CONSIDERATIONS:** None.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# data wrangling/analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# zwift file mgmt\n",
    "import fitdecode\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Union, Optional,Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from Zwift is exports as a *FIT* file that contains the following variables:\n",
    "\n",
    "Field | Variables\n",
    "---|---------\n",
    "`file_id` | serial_number, time_created, manufacturer, product, number, type\n",
    "`device_info` | timestamp, serial_number, cum_operating_time, manufacturer, product, software_version, battery_voltage, device_index, device_type, hardware_version, battery_status\n",
    "`event` | timestamp, timer_trigger, data16, event, event_type, event_group\n",
    "`record` | timestamp, position_lat, position_long, distance, time_from_course, speed, distance, compressed_speed_distance, heart_rate, enhanced_altitude, altitude, enhanced_speed, power, grade, cadence, resistance, cycle_length, temperature\n",
    "`lap` | timestamp, start_time, start_position_lat, start_position_long, end_position_lat, end_position_long, total_elapsed_time, total_timer_time, total_distance, total_strokes, message_index, total_calories, total_fat_calories, enhanced_avg_speed, avg_speed, enhanced_max_speed, max_speed, avg_power, max_power, total_ascent, total_descent, event, event_type, avg_heart_rate, max_heart_rate, avg_cadence, max_cadence, intensity, lap_trigger, sport, event_group\n",
    "`session` | timestamp, start_time, start_position_lat, start_position_long, total_elapsed_time, total_timer_time, total_distance, total_strokes, nec_lat, nec_long, swc_lat, swc_long, message_index, total_calories, total_fat_calories, enhanced_avg_speed, avg_speed, enhanced_max_speed, max_speed, avg_power, max_power, total_ascent, total_descent, first_lap_index, num_laps, event, event_type, sport, sub_sport, avg_heart_rate, max_heart_rate, avg_cadence, max_cadence, total_training_effect, event_group, trigger\n",
    "`activity` | timestamp, total_timer_time, local_timestamp, num_sessions, type, event, event_type, event_group\n",
    "\n",
    "\n",
    "**Note:** There is a `record` field for each second of data recorded. There is also an `event` field at the beginning and end of the `record` fields. \n",
    "<br><br>\n",
    "\n",
    "Data in these fields can be accessed using the `fitdecode` argument `get_values`. Most of the fields are either redundant or irrelevant to me right now. The most relevant will likely be `records` or `session`. Summary statistics, like those reported in `session`, can be calculated from the second-by-second `record` field data. Because of that, I'll only be extracting data from `record`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define a few functions that will help us parse the imported data. I followed [this tutorial](https://towardsdatascience.com/parsing-fitness-tracker-data-with-python-a59e7dc17418) and tweaked [the accompanying code](https://github.com/bunburya/fitness_tracker_data_parsing/blob/main/parse_fit.py) to parse the *FIT* files downloaded from Zwift. Note that missing information is coded as a null value in the final dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for the second-by-second data dataframe\n",
    "RECORD_COLUMN_NAMES = ['timestamp', 'speed', 'distance', 'altitude', 'power', 'grade', 'cadence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_data(frame: fitdecode.records.FitDataMessage) -> Dict[str, Union[float, datetime, timedelta, int]]:\n",
    "    \"\"\"\n",
    "    Extract some data from a FIT frame representing a record and return it as a dictiontary\n",
    "    \"\"\"\n",
    "    data: Dict[str, Union[float, datetime, timedelta, int]] = {}\n",
    "    \n",
    "    for field in RECORD_COLUMN_NAMES:  \n",
    "        if frame.has_field(field):\n",
    "            data[field] = frame.get_value(field)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(fname: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Takes the path to a FIT file (as a string) and returns a DataFrame with data from each record.\n",
    "    \"\"\"\n",
    "    record_data = []\n",
    "    with fitdecode.FitReader(fname) as fit_file:\n",
    "        for frame in fit_file:\n",
    "            if isinstance(frame, fitdecode.records.FitDataMessage):\n",
    "                if frame.name == 'record':\n",
    "                    single_record_data = get_record_data(frame)\n",
    "                    record_data.append(single_record_data)\n",
    "\n",
    "    record_df = pd.DataFrame(record_data, columns=RECORD_COLUMN_NAMES)\n",
    "    record_df.reset_index(inplace=True)\n",
    "    \n",
    "    record_df.rename(columns={'index':'second'}, inplace = True)\n",
    "    \n",
    "    return record_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I loop through the downloaded *FIT* files located on the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to .fit file \n",
    "dir_name = os.getcwd() + '/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['2022-07-12-07-31-42.fit', '2022-07-16-07-47-29.fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      second                 timestamp speed  distance  altitude  power grade  \\\n",
      "0          0 2022-07-12 11:31:53+00:00  None      2.41       6.2     45  None   \n",
      "1          1 2022-07-12 11:31:54+00:00  None      4.28       6.2     45  None   \n",
      "2          2 2022-07-12 11:31:55+00:00  None      6.67       6.0     79  None   \n",
      "3          3 2022-07-12 11:31:56+00:00  None      9.82       6.0     79  None   \n",
      "4          4 2022-07-12 11:31:57+00:00  None     13.12       6.0     79  None   \n",
      "...      ...                       ...   ...       ...       ...    ...   ...   \n",
      "1239    1239 2022-07-12 11:52:32+00:00  None  10950.25       1.6     35  None   \n",
      "1240    1240 2022-07-12 11:52:33+00:00  None  10958.71       1.6     23  None   \n",
      "1241    1241 2022-07-12 11:52:34+00:00  None  10967.59       1.6     23  None   \n",
      "1242    1242 2022-07-12 11:52:35+00:00  None  10976.01       1.6     15  None   \n",
      "1243    1243 2022-07-12 11:52:36+00:00  None  10984.27       1.6     15  None   \n",
      "\n",
      "      cadence  \n",
      "0          24  \n",
      "1          44  \n",
      "2          47  \n",
      "3          49  \n",
      "4          48  \n",
      "...       ...  \n",
      "1239       25  \n",
      "1240       65  \n",
      "1241       65  \n",
      "1242       65  \n",
      "1243       64  \n",
      "\n",
      "[1244 rows x 8 columns]\n",
      "      second                 timestamp speed  distance  altitude  power grade  \\\n",
      "0          0 2022-07-16 11:47:43+00:00  None      2.40      12.8     78  None   \n",
      "1          1 2022-07-16 11:47:44+00:00  None      4.17      12.8     78  None   \n",
      "2          2 2022-07-16 11:47:45+00:00  None      6.39      12.8     78  None   \n",
      "3          3 2022-07-16 11:47:46+00:00  None      8.98      12.8     92  None   \n",
      "4          4 2022-07-16 11:47:47+00:00  None     11.92      12.8     92  None   \n",
      "...      ...                       ...   ...       ...       ...    ...   ...   \n",
      "2640    2640 2022-07-16 12:31:46+00:00  None  22896.36      13.8      0  None   \n",
      "2641    2641 2022-07-16 12:31:47+00:00  None  22897.31      13.8      0  None   \n",
      "2642    2642 2022-07-16 12:31:48+00:00  None  22897.57      13.8      0  None   \n",
      "2643    2643 2022-07-16 12:31:49+00:00  None  22897.57      13.8      0  None   \n",
      "2644    2644 2022-07-16 12:31:50+00:00  None  22897.57      13.8      0  None   \n",
      "\n",
      "      cadence  \n",
      "0          22  \n",
      "1          49  \n",
      "2          49  \n",
      "3          49  \n",
      "4          49  \n",
      "...       ...  \n",
      "2640        0  \n",
      "2641        0  \n",
      "2642        0  \n",
      "2643        0  \n",
      "2644        0  \n",
      "\n",
      "[2645 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# loop through list of FIT files \n",
    "for file in files:\n",
    "    file_path = dir_name + file\n",
    "    record_df = get_dataframe(file_path) \n",
    "    \n",
    "    print(record_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
